{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# How to Use UMAP\n",
    "\n",
    "This Jupyter Notebook tutorial has been converted from the original, found [here](https://umap-learn.readthedocs.io/en/latest/basic_usage.html).\n",
    "\n",
    "UMAP is a general purpose manifold learning and dimension reduction\n",
    "algorithm. It is designed to be compatible with\n",
    "`scikit-learn <http://scikit-learn.org/stable/index.html>`__, making use\n",
    "of the same API and able to be added to sklearn pipelines. If you are\n",
    "already familiar with sklearn you should be able to use UMAP as a drop\n",
    "in replacement for t-SNE and other dimension reduction classes. If you\n",
    "are not so familiar with sklearn this tutorial will step you through the\n",
    "basics of using UMAP to transform and visualise data.\n",
    "\n",
    "First we'll need to import a bunch of useful tools. We will need numpy\n",
    "obviously, but we'll use some of the datasets available in sklearn, as\n",
    "well as the ``train_test_split`` function to divide up data. Finally\n",
    "we'll need some plotting tools (matplotlib and seaborn) to help us\n",
    "visualise the results of UMAP, and pandas to make that a little easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "source": [
    "Penguin data\n",
    "------------\n",
    "\n",
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png\" alt=\"Penguins\" style=\"width: 400px;\" align=\"center\"/>\n",
    "\n",
    "The next step is to get some data to work with. To ease us into things\n",
    "we'll start with the `penguin\n",
    "dataset <https://github.com/allisonhorst/penguins>`. It isn't very\n",
    "representative of what real data would look like, but it is small both\n",
    "in number of points and number of features, and will let us get an idea\n",
    "of what the dimension reduction is doing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\")\n",
    "penguins.head()"
   ]
  },
  {
   "source": [
    "Since this is for demonstration purposes we will get rid of the NaNs in\n",
    "the data; in a real world setting one would wish to take more care with\n",
    "proper handling of missing data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "penguins.species_short.value_counts()"
   ]
  },
  {
   "source": [
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png\" alt=\"Diagram of culmen measurements on a penguin\" style=\"width: 400px;\" align=\"center\"/>\n",
    "\n",
    "See the `github repostiory <https://github.com/allisonhorst/penguins>`__\n",
    "for more details about the dataset itself. It consists of measurements\n",
    "of bill (culmen) and flippers and weights of three species of penguins,\n",
    "along with some other metadata about the penguins. In total we have 334\n",
    "different penguins measured. Visualizing this data is a little bit\n",
    "tricky since we can't plot in 4 dimensions easily. Fortunately four is\n",
    "not that large a number, so we can just to a pairwise feature\n",
    "scatterplot matrix to get an ideas of what is going on. Seaborn makes\n",
    "this easy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguins, hue='species_short')"
   ]
  },
  {
   "source": [
    "This gives us some idea of what the data looks like by giving us all the\n",
    "2D views of the data. Four dimensions is low enough that we can (sort\n",
    "of) reconstruct what the full dimensional data looks like in our heads.\n",
    "Now that we sort of know what we are looking at, the question is what\n",
    "can a dimension reduction technique like UMAP do for us? By reducing the\n",
    "dimension in a way that preserves as much of the structure of the data\n",
    "as possible we can get a visualisable representation of the data\n",
    "allowing us to \"see\" the data and its structure and begin to get some\n",
    "intuition about the data itself.\n",
    "\n",
    "To use UMAP for this task we need to first construct a UMAP object that\n",
    "will do the job for us. That is as simple as instantiating the class. So\n",
    "let's import the umap library and do that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "source": [
    "Before we can do any work with the data it will help to clean up it a\n",
    "little. We won't need NaNs, we just want the measurement columns, and\n",
    "since the measurements are on entirely different scales it will be\n",
    "helpful to convert each feature into z-scores (number of standard\n",
    "deviations from the mean) for comparability."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_data = penguins[[\n",
    "        \"culmen_length_mm\",\n",
    "        \"culmen_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "    ]].values\n",
    "scaled_penguin_data = StandardScaler().fit_transform(penguin_data)"
   ]
  },
  {
   "source": [
    "Now we need to train our reducer, letting it learn about the manifold.\n",
    "For this UMAP follows the sklearn API and has a method ``fit`` which we\n",
    "pass the data we want the model to learn from. Since, at the end of the\n",
    "day, we are going to want to reduced representation of the data we will\n",
    "use, instead, the ``fit_transform`` method which first calls ``fit`` and\n",
    "then returns the transformed data as a numpy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.fit_transform(scaled_penguin_data)\n",
    "embedding.shape"
   ]
  },
  {
   "source": [
    "The result is an array with 334 samples, but only two feature columns\n",
    "(instead of the four we started with). This is because, by default, UMAP\n",
    "reduces down to 2D. Each row of the array is a 2-dimensional\n",
    "representation of the corresponding penguin. Thus we can plot the\n",
    "``embedding`` as a standard scatterplot and color by the target array\n",
    "(since it applies to the transformed data which is in the same order as\n",
    "the original)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in penguins.species_short.map({\"Adelie\":0, \"Chinstrap\":1, \"Gentoo\":2})])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Penguin dataset', fontsize=24)"
   ]
  },
  {
   "source": [
    "This does a useful job of capturing the structure of the data, and as\n",
    "can be seen from the matrix of scatterplots this is relatively accurate.\n",
    "Of course we learned at least this much just from that matrix of\n",
    "scatterplots -- which we could do since we only had four different\n",
    "dimensions to analyse. If we had data with a larger number of dimensions\n",
    "the scatterplot matrix would quickly become unwieldy to plot, and far\n",
    "harder to interpret. So moving on from the Penguin dataset, let's consider\n",
    "the digits dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Digits data\n",
    "-----------\n",
    "\n",
    "First we will load the dataset from sklearn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "source": [
    "We can plot a number of the images to get an idea of what we are looking\n",
    "at. This just involves matplotlib building a grid of axes and then\n",
    "looping through them plotting an image into each one in turn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_array = plt.subplots(20, 20)\n",
    "axes = ax_array.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray_r')\n",
    "plt.setp(axes, xticks=[], yticks=[], frame_on=False)\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.01)"
   ]
  },
  {
   "source": [
    "As you can see these are quite low resolution images -- for the most\n",
    "part they are recognisable as digits, but there are a number of cases\n",
    "that are sufficiently blurred as to be questionable even for a human to\n",
    "guess at. The zeros do stand out as the easiest to pick out as notably\n",
    "different and clearly zeros. Beyond that things get a little harder:\n",
    "some of the squashed thing eights look awfully like ones, some of the\n",
    "threes start to look a little like crossed sevens when drawn badly, and\n",
    "so on.\n",
    "\n",
    "Each image can be unfolded into a 64 element long vector of grayscale\n",
    "values. It is these 64 dimensional vectors that we wish to analyse: how\n",
    "much of the digits structure can we discern? At least in principle 64\n",
    "dimensions is overkill for this task, and we would reasonably expect\n",
    "that there should be some smaller number of \"latent\" features that would\n",
    "be sufficient to describe the data reasonably well. We can try a\n",
    "scatterplot matrix -- in this case just of the first 10 dimensions so\n",
    "that it is at least plottable, but as you can quickly see that approach\n",
    "is not going to be sufficient for this data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(digits.data[:,1:11])\n",
    "digits_df['digit'] = pd.Series(digits.target).map(lambda x: 'Digit {}'.format(x))\n",
    "sns.pairplot(digits_df, hue='digit', palette='Spectral')"
   ]
  },
  {
   "source": [
    "In contrast we can try using UMAP again. It works exactly as before:\n",
    "construct a model, train the model, and then look at the transformed\n",
    "data. To demonstrate more of UMAP we'll go about it differently this\n",
    "time and simply use the ``fit`` method rather than the ``fit_transform``\n",
    "approach we used for Penguins."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(a=None, angular_rp_forest=False, b=None,\n",
    "     force_approximation_algorithm=False, init='spectral', learning_rate=1.0,\n",
    "     local_connectivity=1.0, low_memory=False, metric='euclidean',\n",
    "     metric_kwds=None, min_dist=0.1, n_components=2, n_epochs=None,\n",
    "     n_neighbors=15, negative_sample_rate=5, output_metric='euclidean',\n",
    "     output_metric_kwds=None, random_state=42, repulsion_strength=1.0,\n",
    "     set_op_mix_ratio=1.0, spread=1.0, target_metric='categorical',\n",
    "     target_metric_kwds=None, target_n_neighbors=-1, target_weight=0.5,\n",
    "     transform_queue_size=4.0, transform_seed=42, unique=False, verbose=False)\n",
    "reducer.fit(digits.data)"
   ]
  },
  {
   "source": [
    "Now, instead of returning an embedding we simply get back the reducer\n",
    "object, now having trained on the dataset we passed it. To access the\n",
    "resulting transform we can either look at the ``embedding_`` attribute\n",
    "of the reducer object, or call transform on the original data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(digits.data)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape"
   ]
  },
  {
   "source": [
    "We now have a dataset with 1797 rows (one for each hand-written digit\n",
    "sample), but only 2 columns. As with the Penguins example we can now plot\n",
    "the resulting embedding, coloring the data points by the class that\n",
    "they belong to (i.e. the digit they represent)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=10)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);"
   ]
  },
  {
   "source": [
    "We see that UMAP has successfully captured the digit classes. There are\n",
    "also some interesting effects as some digit classes blend into one\n",
    "another (see the eights, ones, and sevens, with some nines in between),\n",
    "and also cases where digits are pushed away as clearly distinct (the\n",
    "zeros on the right, the fours at the top, and a small subcluster of ones\n",
    "at the bottom come to mind). To get a better idea of why UMAP chose to\n",
    "do this it is helpful to see the actual digits involve. One can do this\n",
    "using [bokeh](https://bokeh.pydata.org/en/latest/) and mouseover\n",
    "tooltips of the images.\n",
    "\n",
    "First we'll need to encode all the images for inclusion in a dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddable_image(data):\n",
    "    img_data = 255 - 15 * data.astype(np.uint8)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()"
   ]
  },
  {
   "source": [
    "Next we need to load up bokeh and the various tools from it that will be\n",
    "needed to generate a suitable interactive plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "source": [
    "Finally we generate the plot itself with a custom hover tooltip that\n",
    "embeds the image of the digit in question in it, along with the digit\n",
    "class that the digit is actually from (this can be useful for digits\n",
    "that are hard even for humans to classify correctly)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "digits_df['digit'] = [str(x) for x in digits.target]\n",
    "digits_df['image'] = list(map(embeddable_image, digits.images))\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "color_mapping = CategoricalColorMapper(factors=[str(9 - x) for x in digits.target_names],\n",
    "                                       palette=Spectral10)\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of the Digits dataset',\n",
    "    plot_width=600,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>Digit:</span>\n",
    "        <span style='font-size: 18px'>@digit</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color=dict(field='digit', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "source": [
    "As can be seen, the nines that blend between the ones and the sevens are\n",
    "odd looking nines (that aren't very rounded) and do, indeed, interpolate\n",
    "surprisingly well between ones with hats and crossed sevens. In contrast\n",
    "the small disjoint cluster of ones at the bottom of the plot is made up\n",
    "of ones with feet (a horizontal line at the base of the one) which are,\n",
    "indeed, quite distinct from the general mass of ones.\n",
    "\n",
    "This concludes our introduction to basic UMAP usage -- hopefully this\n",
    "has given you the tools to get started for yourself. Further tutorials,\n",
    "covering UMAP parameters and more advanced usage are also available when\n",
    "you wish to dive deeper."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic UMAP Usage and Parameters\n",
    "\n",
    "UMAP is a fairly flexible non-linear dimension reduction algorithm. It seeks to learn the manifold structure of your data and find a low dimensional embedding that preserves the essential topological structure of that manifold. In this notebook we will generate some visualisable 4-dimensional data, demonstrate how to use UMAP to provide a 2-dimensional representation of it, and then look at how various UMAP parameters can impact the resulting embedding. This notebook is based on the work of Philippe Rivière for visionscarto.net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we'll need some basic libraries. First ``numpy`` will be needed for basic array manipulation. Since we will be visualising the results we will need ``matplotlib`` and ``seaborn``. Finally we will need ``umap`` for doing the dimension reduction itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import umap\n",
    "%matplotlib inline\n",
    "sns.set(style='white', rc={'figure.figsize':(10,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need some data to embed into a lower dimensional representation. To make the 4-dimensional data \"visualisable\" we will generate data uniformly at random from a 4-dimensional cube such that we can interpret a sample as a tuple of (R,G,B,a) values specifying a color (and translucency). Thus when we plot low dimensional representations each point can colored according to its 4-dimensional value. For this we can use ``numpy``. We will fix a random seed for the sake of consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(800, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find a low dimensional representation of the data. The ``umap`` library provides a ``UMAP`` class that is fully compatible with ``scikit-learn``. Thus, if you are familiar with ``scikit-learn`` the following will seem natural. For those no faimilar with the excellent ``scikit-learn`` library, the first step is to instantiate an object of the ``UMAP`` class (the hyperparameters for the model are set at this stage, as we will see shortly); that object then provides some standard methods, notably ``fit`` and ``fit_transform``. These methods take a dataset, and fit the model to the data. In the case of ``fit_transform`` the return value is the transformation of the data under the resulting fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = umap.UMAP()\n",
    "%time u = fit.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting value ``u`` is a 2-dimensional representation of the data. We can visualise the result by using ``matplotlib`` to draw a scatter plot of ``u``. We can color each point of the scatter plot by the associated 4-dimensional color from the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u[:,0], u[:,1], c=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the result is that the data is placed in 2-dimensional space such that points that were nearby in in 4-dimensional space (i.e. are similar colors) are kept close together. Since we drew a random selection of points in the color sube there is a certain amount of induced structure from where the random points happened to clump up in color space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Selection\n",
    "\n",
    "UMAP has several hyperparameters that can have a significant impact on the resulting embedding. In this notebook we will be covering the four major ones:\n",
    "\n",
    " - ``n_neighbors``\n",
    " - ``min_dist``\n",
    " - ``n_components``\n",
    " - ``metric``\n",
    " \n",
    "Each of these parameters has a distinct effect, and we will look at each in turn. To make exploration simpler we will first write a short utility function that can fit the data with UMAP given a set of parameter choices, and plot the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_umap(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    u = fit.fit_transform(data);\n",
    "    fig = plt.figure()\n",
    "    if n_components == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], range(len(u)), c=data)\n",
    "    if n_components == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], u[:,1], c=data)\n",
    "    if n_components == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(u[:,0], u[:,1], u[:,2], c=data)\n",
    "    plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``n_neighbors``\n",
    "\n",
    "This parameter controls how UMAP balances local versus global structure in the data. It does this by constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data. This means that low values of ``n_neighbors`` will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point when estimating the manifold structure of the data, loosing fine detail structure for the sake of getting the broader of the data.\n",
    "\n",
    "We can see that in practice by fitting our dataset with UMAP using a range of ``n_neighbors`` values. The default value of ``n_neighbors`` for UMAP (as used above) is 15, but we will look at values ranging from 2 (a very local view of the manifold) up to 200 (a quarter of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (2, 5, 10, 20, 50, 100, 200):\n",
    "    draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a value of ``n_neighbors=2`` we see that UMAP merely glues together small chains, but due to the narrow/local view, fails to see how those connect together. It also leaves many different components (and even singleton points). This represents the fact that from a fine detail point of view the data is very disconnected and scattered throughout the space.\n",
    "\n",
    "As ``n_neighbors`` is increased UMAP manages to see more of the overall structure of the data, gluing more components together, and better coverying the broader structure of the data. By the stage of ``n_neighbors=20`` we have a fairly good overall view of the data showing how the various colors interelate to each other over the whole dataset.\n",
    "\n",
    "As ``n_neighbors`` increases further more and more focus in placed on the overall structure of the data. This results in, with ``n_neighbors=200`` a plot where the overall structure (blues, greens, and reds; high luminance versus low) is well captured, but at the loss of some of the finer local sturcture (individual colors are no longer necessarily immediately near their closest color match).\n",
    "\n",
    "This effect well exemplifies the local/global tradeoff provided by ``n_neighbors``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``min_dist``\n",
    "\n",
    "The ``min_dist`` parameter controls how tightly UMAP is allowed to pack points together. It, quite literally, provides the minimum distance apart that points are allowed to be in the low dimensional representation. This means that low values of ``min_dist`` will result in clumpier embeddings. This can be useful if you are interested in clustering, or in finer topological structure.  Larger values of ``min_dist`` will prevent UMAP from packing point together and will focus instead on the preservation of the broad topological structure instead.\n",
    "\n",
    "The default value for ``min_dist`` (as used above) is 0.1. We will look at a range of values from 0.0 through to 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "for d in (0.0, 0.1, 0.25, 0.5, 0.8, 0.99):\n",
    "    draw_umap(min_dist=d, title='min_dist = {}'.format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that with ``min_dist=0.0`` UMAP manages to find small connected components, clumps and strings in the data, and emphasises these features in the resulting embedding. As ``min_dist`` is increased these structures are pushed apart into softer more general features, providing a better overarching view of the data at the loss of the more detailed topological structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``n_components``\n",
    "\n",
    "As is standard for many ``scikit-learn`` dimension reduction algorithms UMAP provides a ``n_components`` parameter option that allows the user to determine the dimensionality of the reduced dimension space we will be embedding the data into. Unlike some other visualisation algorithms such as t-SNE UMAP scales well in embedding dimension, so you can use it for more than just visualisation in 2- or 3-dimensions.\n",
    "\n",
    "For the purposes of this demonstration (so that we can see the effects of the parameter) we will only be looking at 1-dimensional and 3-dimensional embeddings, which we have some hope of visualizing.\n",
    "\n",
    "First of all we will set ``n_components`` to 1, forcing UMAP to embed the data in a line. For visualisation purposes we will randomly distribute the data on the y-axis to provide some separation between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "draw_umap(n_components=1, title='n_components = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try ``n_components=3``. For visualisation we will make use of ``matplotlib``'s basic 3-dimensional plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "draw_umap(n_components=3, title='n_components = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that with more dimensions in which to work UMAP has an easier time separating out the colors in a way that respects the topological structure of the data.\n",
    "\n",
    "As mentioned, there is really no requirement to stop at ``n_components`` at 3. If you are interested in (density based) clustering, or other machine learning techniques, it can be beneficial to pick a larger embedding dimension (say 10, or 50) closer to the the dimension of the underlying manifold on which your data lies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``metric``\n",
    "\n",
    "The final UMAP parameter we will be considering in this notebook is the ``metric`` parameter. This controls how distance is computed in the ambient space of the input data. By default UMAP supports a wide variety of metrics, including:\n",
    "\n",
    "**Minkowski style metrics**\n",
    "* euclidean\n",
    "* manhattan\n",
    "* chebyshev\n",
    "* minkowski\n",
    "\n",
    "**Miscellaneous spatial metrics**\n",
    "* canberra\n",
    "* braycurtis\n",
    "* haversine\n",
    "\n",
    "**Normalized spatial metrics**\n",
    "* mahalanobis\n",
    "* wminkowski\n",
    "* seuclidean\n",
    "\n",
    "**Angular and correlation metrics**\n",
    "* cosine\n",
    "* correlation\n",
    "\n",
    "**Metrics for binary data**\n",
    "* hamming\n",
    "* jaccard\n",
    "* dice\n",
    "* russelrao\n",
    "* kulsinski\n",
    "* rogerstanimoto\n",
    "* sokalmichener\n",
    "* sokalsneath\n",
    "* yule\n",
    "\n",
    "Any of which can be specified by setting ``metric='<metric name>'``; for example to use cosine distance as the metric you would use ``metric='cosine'``.\n",
    "\n",
    "UMAP offers more than this however -- it supports custom user defined metrics as long as those metrics can be compiled in ``nopython`` mode by numba. For this notebook we will be looking at such custom metrics. To define such metrics we'll need numba ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first custom metric we'll define the distance to be the absolute value of difference in the red channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def red_channel_dist(a,b):\n",
    "    return np.abs(a[0] - b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more adventurous it will be useful to have some colorspace conversion -- to keep things simple we'll just use HSL formulas to extract the hue, saturation, and lightness from an (R,G,B) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def hue(r, g, b):\n",
    "    cmax = max(r, g, b)\n",
    "    cmin = min(r, g, b)\n",
    "    delta = cmax - cmin\n",
    "    if cmax == r:\n",
    "        return ((g - b) / delta) % 6\n",
    "    elif cmax == g:\n",
    "        return ((b - r) / delta) + 2\n",
    "    else:\n",
    "        return ((r - g) / delta) + 4\n",
    "    \n",
    "@numba.njit()\n",
    "def lightness(r, g, b):\n",
    "    cmax = max(r, g, b)\n",
    "    cmin = min(r, g, b)\n",
    "    return (cmax + cmin) / 2.0\n",
    "\n",
    "@numba.njit()\n",
    "def saturation(r, g, b):\n",
    "    cmax = max(r, g, b)\n",
    "    cmin = min(r, g, b)\n",
    "    chroma = cmax - cmin\n",
    "    light = lightness(r, g, b)\n",
    "    if light == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return chroma / (1 - abs(2*light - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in hand we can define three extra distances. The first simply measures the difference in hue, the second measures the euclidean distance in a combined saturation and lightness space, while the third measures distance in the full HSL space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def hue_dist(a, b):\n",
    "    diff = (hue(a[0], a[1], a[2]) - hue(b[0], b[1], b[2])) % 6\n",
    "    if diff < 0:\n",
    "        return diff + 6\n",
    "    else:\n",
    "        return diff\n",
    "\n",
    "@numba.njit()\n",
    "def sl_dist(a, b):\n",
    "    a_sat = saturation(a[0], a[1], a[2])\n",
    "    b_sat = saturation(b[0], b[1], b[2])\n",
    "    a_light = lightness(a[0], a[1], a[2])\n",
    "    b_light = lightness(b[0], b[1], b[2])\n",
    "    return (a_sat - b_sat)**2 + (a_light - b_light)**2\n",
    "\n",
    "@numba.njit()\n",
    "def hsl_dist(a, b):\n",
    "    a_sat = saturation(a[0], a[1], a[2])\n",
    "    b_sat = saturation(b[0], b[1], b[2])\n",
    "    a_light = lightness(a[0], a[1], a[2])\n",
    "    b_light = lightness(b[0], b[1], b[2])\n",
    "    a_hue = hue(a[0], a[1], a[2])\n",
    "    b_hue = hue(b[0], b[1], b[2])\n",
    "    return (a_sat - b_sat)**2 + (a_light - b_light)**2 + (((a_hue - b_hue) % 6) / 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such custom metrics in hand we can get UMAP to embed the data using those metrics to measure distance between our input data points. Note that ``numba`` provides significant flexibility in what we can do in defining distance functions. Despite this we retain the high performance we expect from UMAP even using such custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "for m in (\"euclidean\", red_channel_dist, sl_dist, hue_dist, hsl_dist):\n",
    "    name = m if type(m) is str else m.__name__\n",
    "    draw_umap(n_components=2, metric=m, title='metric = {}'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can see the effects of the metrics quite clearly. The pure red channel correctly see the data as living on a one dimensional manifold, the hue metric interprets the data as living in a circle, and the HSL metric fattens out the circle according to the saturation and lightness. This provides a reasonable demonstration of the power and flexibility of UMAP in understanding the underlying topology of data, and finding a suitable low dimensional representation of that topology."
   ]
  }  
 ]
}